# ğŸš— RAG Multimodal MetaCLIP 2 - Dataset Balanceado con Contexto Enriquecido

## ğŸ“‹ Ãndice

1. [VisiÃ³n General](#-visiÃ³n-general)
2. [Arquitectura del Sistema](#-arquitectura-del-sistema)
3. [Dataset](#-dataset)
4. [MÃ³dulos del Sistema](#-mÃ³dulos-del-sistema)
5. [Pipeline de ImplementaciÃ³n](#-pipeline-de-implementaciÃ³n)
6. [Estructura de Archivos](#-estructura-de-archivos)
7. [Esquema de Metadata](#-esquema-de-metadata)
8. [ConfiguraciÃ³n y ParÃ¡metros](#-configuraciÃ³n-y-parÃ¡metros)
9. [Resultados Esperados](#-resultados-esperados)
10. [PrÃ³ximos Pasos](#-prÃ³ximos-pasos)

---

## ğŸ¯ VisiÃ³n General

### Objetivo

Implementar un sistema RAG (Retrieval-Augmented Generation) multimodal que procese un dataset balanceado de imÃ¡genes vehiculares con y sin daÃ±os, utilizando **MetaCLIP 2** para generar embeddings unificados (visual + textual) que preserven la mÃ¡xima informaciÃ³n y contexto de las imÃ¡genes originales.

### CaracterÃ­sticas Principales

- âœ… **Dataset Balanceado**: 50% imÃ¡genes con daÃ±o (ko) + 50% sin daÃ±o (ok)
- âœ… **Embeddings Unificados con MetaCLIP 2**: Visual + Textual en espacio compartido (1024d)
- âœ… **ResoluciÃ³n Optimizada**: 448Ã—448 con estrategia multi-patch para preservar detalles
- âœ… **Sin PÃ©rdida de InformaciÃ³n**: Eliminados procesados que degradan calidad visual
- âœ… **Contexto Enriquecido**: Descripciones textuales ricas con zona, parte especÃ­fica y tipos de daÃ±o
- âœ… **Ãndice Ãšnico FAISS**: UnificaciÃ³n de crops damaged + clean con filtros avanzados
- âœ… **Crops Inteligentes**: Clusterizados para daÃ±os, grid adaptativo para imÃ¡genes limpias

### Cambios Clave vs. Arquitectura Anterior

| Aspecto | Antes (DINOv3+BERT) | Ahora (MetaCLIP 2) |
|---------|--------------------|--------------------|
| **Modelo de embeddings** | 2 modelos separados | âœ… **1 modelo unificado** |
| **DimensiÃ³n** | 1408d (concatenados) | âœ… **1024d (espacio compartido)** |
| **AlineaciÃ³n visual-texto** | Manual (weights 50/50) | âœ… **Nativa end-to-end** |
| **Velocidad** | Lenta (2 forward pass) | âœ… **2Ã— mÃ¡s rÃ¡pida** |
| **ResoluciÃ³n crops** | 448Ã—448 fijo | âœ… **448Ã—448 + multi-patch** |
| **PÃ©rdida informaciÃ³n** | Padding gris | âœ… **Sin artifacts** |
| **VRAM requerida** | ~8GB | âœ… **~6GB** |

---

## ğŸ—ï¸ Arquitectura del Sistema

```
Dataset Split 2 (552 imÃ¡genes)
           â†“
    Â¿Tiene daÃ±o?
     â†™         â†˜
   SÃ          NO
(276 ko)    (276 ok)
    â†“            â†“
Clustered    Vehicle Detector
  Crop       + Grid Crop
Generator      Generator
    â†“            â†“
~850 crops   ~1,500-2,000 crops
(448Ã—448)       (448Ã—448)
    â†“            â†“
Damage       Clean
Contextualizer  Contextualizer
    â†“            â†“
Contexto     Contexto
CON daÃ±o     SIN daÃ±o
    â†“            â†“
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â†“
   MetaCLIP 2 Embedder
   (Fusion: Average)
          â†“
   Embeddings Unificados
      (1024 dims)
          â†“
   Ãndice FAISS Unificado
    (IndexHNSWFlat)
          â†“
  Retriever con Filtros
  (zona + has_damage)
```

---

## ğŸ“Š Dataset

### Estructura del Dataset Split 2

```
data/raw/dataset_split_2/
â”œâ”€â”€ ImÃ¡genes CON daÃ±o (276 imÃ¡genes)
â”‚   â”œâ”€â”€ zona1_ko_2_3_1554817134014_zona_4_imageDANO_original.jpg
â”‚   â”œâ”€â”€ zona1_ko_2_3_1554817134014_zona_4_imageDANO_original.json          # Ground truth
â”‚   â””â”€â”€ zona1_ko_2_3_1554817134014_zona_4_labelDANO_modificado.json        # SegmentaciÃ³n
â”‚
â””â”€â”€ ImÃ¡genes SIN daÃ±o (276 imÃ¡genes)
    â”œâ”€â”€ zona1_ok_2_3_1554373063646_zona_6_imageDANO_original.jpg
    â””â”€â”€ zona1_ok_2_3_1554373063646_zona_6_imageDANO_original.json          # Ground truth "No damage"
```

### CaracterÃ­sticas

| Aspecto | Valor |
|---------|-------|
| **Total imÃ¡genes** | 552 |
| **Con daÃ±o (ko)** | 276 (50%) |
| **Sin daÃ±o (ok)** | 276 (50%) |
| **Formato** | JPG |
| **Anotaciones** | JSON (LabelMe format) |

### Naming Convention

```
zona1_{ko|ok}_2_3_{timestamp}_zona_{1-10}_imageDANO_original.jpg
  â”‚     â”‚                         â”‚
  â”‚     â”‚                         â””â”€ Zona del vehÃ­culo (1-10)
  â”‚     â””â”€ ko: con daÃ±o | ok: sin daÃ±o
  â””â”€ Identificador del dataset
```

### Zonas del VehÃ­culo

| Zona | Nombre TÃ©cnico | DescripciÃ³n | Ãrea |
|------|----------------|-------------|------|
| 1 | front_left_fender | Guardabarros delantero izquierdo | frontal |
| 2 | hood_center | CapÃ³ central | frontal |
| 3 | front_right_fender | Guardabarros delantero derecho | frontal |
| 4 | rear_left_quarter | Panel trasero izquierdo | posterior |
| 5 | rear_bumper | Parachoques trasero | posterior |
| 6 | rear_right_quarter | Panel trasero derecho | posterior |
| 7 | driver_side_door | Puerta del conductor (izquierda) | lateral_left |
| 8 | driver_side_rocker | Panel lateral del conductor | lateral_left |
| 9 | passenger_side_door | Puerta del pasajero (derecha) | lateral_right |
| 10 | passenger_side_rocker | Panel lateral del pasajero | lateral_right |

### Tipos de DaÃ±o

| Label | Nombre CanÃ³nico | DescripciÃ³n |
|-------|-----------------|-------------|
| 1 | surface_scratch | AraÃ±azo superficial |
| 2 | dent | Abolladura |
| 3 | paint_peeling | Desprendimiento de pintura |
| 4 | deep_scratch | AraÃ±azo profundo |
| 5 | crack | Grieta |
| 6 | missing_part | Parte faltante |
| 7 | missing_accessory | Accesorio faltante |
| 8 | misaligned_part | Parte desalineada |

---

## ğŸ§© MÃ³dulos del Sistema

### 1. Vehicle Detector ğŸš—

**UbicaciÃ³n**: `src/core/preprocessing/vehicle_detector.py`

**FunciÃ³n**: Detectar el coche principal en imÃ¡genes sin daÃ±o para centrar los crops.

**TecnologÃ­a**: YOLOv8 pre-entrenado en COCO (clase 'car')

**Proceso**:
1. Detectar todos los coches en la imagen
2. Seleccionar el bbox mÃ¡s grande (coche principal)
3. Expandir bbox con margen (10-15%) para contexto
4. Retornar coordenadas ajustadas

**Output**:
```python
{
    'bbox': [x1, y1, x2, y2],
    'confidence': 0.95,
    'vehicle_area_ratio': 0.68  # % de imagen ocupado
}
```

**Ventajas**:
- âœ… Centra crops en el vehÃ­culo
- âœ… Minimiza fondo innecesario
- âœ… Mejora calidad de embeddings visuales

---

### 2. Grid Crop Generator ğŸ“

**UbicaciÃ³n**: `src/core/preprocessing/grid_crop_generator.py`

**FunciÃ³n**: Generar grid de crops 448Ã—448 para imÃ¡genes sin daÃ±o.

**Estrategia**:
- Sliding window dentro del bbox del vehÃ­culo
- Overlap inteligente (20-30%) para cobertura completa
- Filtrado: solo crops con >70% Ã¡rea del coche

**ParÃ¡metros**:
```python
crop_size = 448  # Ã“ptimo para MetaCLIP
overlap = 0.25   # 25%
min_vehicle_ratio = 0.70  # MÃ­nimo 70% del crop debe ser coche
```

**Â¿Por quÃ© 448Ã—448?**
- âœ… Compatible con MetaCLIP H/14 (336Ã—336 nativo)
- âœ… Preserva mÃ¡s detalles que 224Ã—224
- âœ… Balance Ã³ptimo: calidad vs. eficiencia computacional
- âœ… Permite multi-patch sin overhead excesivo

**Output Esperado por Imagen**:
- ImÃ¡genes grandes: 6-10 crops
- ImÃ¡genes medianas: 4-6 crops
- ImÃ¡genes pequeÃ±as: 2-4 crops

**Total Estimado**: ~1,500-2,000 crops para 276 imÃ¡genes ok

---

### 3. Clustered Crop Generator (Optimizado) ğŸ”§

**UbicaciÃ³n**: `src/core/preprocessing/clustered_crop_generator_optimized.py`

**Cambios CrÃ­ticos para Preservar InformaciÃ³n**:

#### âŒ **ELIMINADO**: Padding Adaptativo con Color Gris

```python
# âŒ ANTES (causaba artifacts y pÃ©rdida de informaciÃ³n)
canvas = np.full((448, 448, 3), [114, 114, 114], dtype=np.uint8)
canvas[y:y+h, x:x+w] = crop  # Insertar crop centrado
```

**Problemas del padding gris**:
- ğŸš« Artifacts visuales confunden al modelo
- ğŸš« Pierde contexto espacial real
- ğŸš« Reduce Ã¡rea efectiva del vehÃ­culo
- ğŸš« Embeddings de menor calidad

#### âœ… **NUEVO**: Resize Proporcional Sin Padding

```python
# âœ… AHORA (preserva toda la informaciÃ³n real)
def generate_crop_optimized(self, bbox, image):
    """
    Genera crop sin padding artificial
    """
    x, y, w, h = bbox
    crop = image[y:y+h, x:x+w]
    
    # OpciÃ³n 1: Si crop es mÃ¡s pequeÃ±o que 448Ã—448
    if w <= 448 and h <= 448:
        # NO hacer padding, usar multi-patch en embedder
        return crop  # Retornar tamaÃ±o original
    
    # OpciÃ³n 2: Si crop es mÃ¡s grande que 448Ã—448
    else:
        # Resize proporcional manteniendo aspecto
        scale = min(448 / w, 448 / h)
        new_w = int(w * scale)
        new_h = int(h * scale)
        
        crop_resized = cv2.resize(
            crop, 
            (new_w, new_h),
            interpolation=cv2.INTER_LANCZOS4  # Mejor calidad
        )
        
        return crop_resized  # Retornar sin padding
```

**Ventajas**:
- âœ… Solo pÃ­xeles reales del vehÃ­culo
- âœ… Sin artifacts artificiales
- âœ… Mejor calidad visual para MetaCLIP
- âœ… Preserva toda la informaciÃ³n de la imagen original

**Manejo de Crops PequeÃ±os**:
Si crop < 448Ã—448, se pasa directamente al embedder que usa **estrategia multi-patch** (ver secciÃ³n MetaCLIP Embedder).

**Output Esperado**: ~850 crops para 276 imÃ¡genes ko

---

### 4. Damage Contextualizer ğŸ“

**UbicaciÃ³n**: `src/core/embeddings/damage_contextualizer.py`

**FunciÃ³n**: Generar descripciones textuales enriquecidas para cada crop.

#### Para ImÃ¡genes CON DaÃ±o

**MÃ©todo**: `build_damage_context(metadata: Dict) -> str`

**Estructura**:
1. **Zona del vehÃ­culo** (del naming de archivo)
2. **Parte especÃ­fica** (inferida con heurÃ­stica o VLM)
3. **Tipos de daÃ±o** con descripciÃ³n breve
4. **RelaciÃ³n espacial** entre daÃ±os

**ImplementaciÃ³n**:

```python
def build_damage_context(self, metadata: Dict) -> str:
    """
    Genera contexto rico para crops con daÃ±o
    """
    zone_desc = metadata['zone_description']  # "rear_left_quarter"
    zone_area = metadata['zone_area']         # "posterior"
    
    # Inferir parte especÃ­fica del vehÃ­culo
    specific_part = self.infer_specific_part(
        zone=zone_desc,
        bbox_center=metadata.get('bbox_center', (0.5, 0.5)),
        damage_types=metadata.get('damage_types', [])
    )
    # Ejemplo: "Rear left corner panel near bumper junction"
    
    # Describir tipos de daÃ±o
    from collections import Counter
    type_counts = Counter(metadata['damage_types'])
    
    damage_descriptions = []
    for dtype, count in type_counts.items():
        friendly = self.get_friendly_description(dtype)
        damage_descriptions.append(
            f"{dtype.replace('_', ' ')} ({friendly}, {count} instance{'s' if count > 1 else ''})"
        )
    
    # Analizar patrÃ³n espacial
    spatial_pattern = self.analyze_spatial_pattern(metadata['defects'])
    
    # Ensamblar contexto
    context = (
        f"Vehicle zone: {zone_desc} ({zone_area} area). "
        f"Affected part: {specific_part}. "
        f"Damage types: {', '.join(damage_descriptions)}. "
        f"Spatial pattern: {spatial_pattern}."
    )
    
    return context

def get_friendly_description(self, damage_type: str) -> str:
    """Descripciones amigables para tipos de daÃ±o"""
    descriptions = {
        'surface_scratch': 'minor surface abrasion',
        'dent': 'metal deformation',
        'paint_peeling': 'paint layer detachment',
        'deep_scratch': 'deep paint penetration',
        'crack': 'structural fracture',
        'missing_part': 'component absence',
        'missing_accessory': 'accessory detachment',
        'misaligned_part': 'panel misalignment'
    }
    return descriptions.get(damage_type, 'unspecified damage')

def analyze_spatial_pattern(self, defects: list) -> str:
    """Analiza patrÃ³n espacial entre defectos"""
    if len(defects) <= 1:
        return "isolated damage"
    
    # Calcular distancias entre defectos
    centers = [
        ((d['bbox'][0] + d['bbox'][2]) / 2, 
         (d['bbox'][1] + d['bbox'][3]) / 2)
        for d in defects
    ]
    
    # HeurÃ­stica simple de clustering
    avg_distance = np.mean([
        np.linalg.norm(np.array(centers[i]) - np.array(centers[j]))
        for i in range(len(centers))
        for j in range(i+1, len(centers))
    ])
    
    if avg_distance < 100:  # PÃ­xeles
        return "defects clustered together, suggesting single impact event"
    else:
        return "defects distributed across area, multiple impact points"

def infer_specific_part(
    self, 
    zone: str, 
    bbox_center: tuple,
    damage_types: list
) -> str:
    """
    Infiere parte especÃ­fica del vehÃ­culo usando heurÃ­stica
    
    Alternativa: Integrar VLM ligero como Florence-2 o PaliGemma
    """
    # Mapa zona â†’ partes posibles
    part_mapping = {
        'front_left_fender': {
            'top': 'Upper front fender panel',
            'center': 'Front fender mid-section',
            'bottom': 'Front fender lower edge near wheel arch'
        },
        'hood_center': {
            'front': 'Front hood edge near grille',
            'center': 'Central hood panel',
            'rear': 'Hood rear section near windshield'
        },
        'rear_left_quarter': {
            'top': 'Upper rear quarter panel',
            'center': 'Rear left corner panel near bumper junction',
            'bottom': 'Lower rear quarter panel near wheel arch'
        },
        # ... (mapeo completo para todas las zonas)
    }
    
    # Determinar posiciÃ³n relativa
    cx, cy = bbox_center
    if cy < 0.33:
        position = 'top'
    elif cy < 0.67:
        position = 'center'
    else:
        position = 'bottom'
    
    parts = part_mapping.get(zone, {})
    return parts.get(position, f"{zone.replace('_', ' ')} panel")
```

**Ejemplo Output**:
```
Vehicle zone: rear_left_quarter (posterior area). 
Affected part: Rear left corner panel near bumper junction. 
Damage types: surface scratch (minor surface abrasion, 2 instances), 
              dent (metal deformation, 1 instance). 
Spatial pattern: defects clustered together, suggesting single impact event.
```

**Longitud**: ~150-200 caracteres

#### Para ImÃ¡genes SIN DaÃ±o

**MÃ©todo**: `build_clean_context(metadata: Dict) -> str`

**Estructura**:
1. Zona del vehÃ­culo
2. Parte especÃ­fica
3. CondiciÃ³n superficie (minimalista)

**ImplementaciÃ³n**:

```python
def build_clean_context(self, metadata: Dict) -> str:
    """
    Contexto minimalista para imÃ¡genes limpias
    """
    zone_desc = metadata['zone_description']
    zone_area = metadata['zone_area']
    
    # Inferir parte especÃ­fica usando posiciÃ³n del grid
    specific_part = self.infer_specific_part(
        zone=zone_desc,
        bbox_center=metadata.get('grid_center', (0.5, 0.5))
    )
    
    context = (
        f"Vehicle zone: {zone_desc} ({zone_area} area). "
        f"Inspected part: {specific_part}. "
        f"Surface condition: Clean paint, no scratches or dents detected. "
        f"Panel integrity: Normal alignment, intact surface."
    )
    
    return context
```

**Ejemplo Output**:
```
Vehicle zone: hood_center (frontal area). 
Inspected part: Central hood panel. 
Surface condition: Clean paint, no scratches or dents detected. 
Panel integrity: Normal alignment, intact surface.
```

**Longitud**: ~120-150 caracteres

---

### 5. MetaCLIP 2 Embedder (Unificado) ğŸ§ 

**UbicaciÃ³n**: `src/core/embeddings/metaclip_embedder_unified.py`

**FunciÃ³n**: Generar embeddings unificados (visual + textual) usando MetaCLIP 2.

#### Ventajas de MetaCLIP 2 sobre DINOv3 + BERT

| CaracterÃ­stica | DINOv3 + BERT | MetaCLIP 2 |
|----------------|---------------|------------|
| **Modelos** | 2 separados | âœ… **1 unificado** |
| **AlineaciÃ³n** | Manual (weights) | âœ… **Nativa end-to-end** |
| **Espacio embedding** | ConcatenaciÃ³n forzada | âœ… **Compartido naturalmente** |
| **DimensiÃ³n** | 1408d (1024+384) | âœ… **1024d** (mÃ¡s eficiente) |
| **Velocidad** | 2 forward pass | âœ… **1 forward pass** |
| **VRAM** | ~8GB | âœ… **~6GB** |
| **Retrieval** | Bueno | âœ… **Superior** |
| **Mantenimiento** | Complejo | âœ… **Simple** |

#### ImplementaciÃ³n Completa

```python
# src/core/embeddings/metaclip_embedder_unified.py

from transformers import AutoProcessor, AutoModel
import torch
import numpy as np
from PIL import Image
from typing import List, Dict, Tuple
from pathlib import Path

class MetaCLIPUnifiedEmbedder:
    """
    Embedder unificado con MetaCLIP 2 para dataset balanceado
    
    CaracterÃ­sticas:
    - Visual + Textual en espacio compartido
    - Multi-patch para preservar detalles
    - FusiÃ³n optimizada (average o weighted)
    """
    
    def __init__(
        self,
        model_name: str = "facebook/metaclip-h14-fullcc2.5b",
        use_multipatch: bool = True,
        patch_size: int = 336,
        patch_stride: int = 224,
        device: str = None
    ):
        """
        Args:
            model_name: MetaCLIP model
                - "facebook/metaclip-b32-400m" â†’ 512d (rÃ¡pido)
                - "facebook/metaclip-h14-fullcc2.5b" â†’ 1024d âœ… (recomendado)
            use_multipatch: Si True, usa estrategia multi-patch para crops grandes
            patch_size: TamaÃ±o de patch para multi-patch (336 recomendado)
            patch_stride: Stride para sliding window (224 â†’ 33% overlap)
        """
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"\n{'='*70}")
        print(f"ğŸ”§ INICIALIZANDO MetaCLIP Unified Embedder")
        print(f"{'='*70}")
        print(f"Model: {model_name}")
        print(f"Device: {self.device}")
        print(f"Multi-patch: {use_multipatch}")
        if use_multipatch:
            print(f"Patch size: {patch_size}Ã—{patch_size}")
            print(f"Patch stride: {patch_stride} (overlap: {(1 - patch_stride/patch_size)*100:.0f}%)")
        print(f"{'='*70}\n")
        
        # Cargar modelo
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        
        # ConfiguraciÃ³n
        self.embedding_dim = 1024 if "h14" in model_name else 512
        self.use_multipatch = use_multipatch
        self.patch_size = patch_size
        self.patch_stride = patch_stride
        
        print(f"âœ… Modelo cargado")
        print(f"   - Embedding dim: {self.embedding_dim}")
        print(f"   - Total params: {sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M\n")
    
    def generate_unified_embedding(
        self,
        image_path: Path,
        text_description: str,
        fusion_strategy: str = "average"
    ) -> np.ndarray:
        """
        Genera embedding unificado (imagen + texto)
        
        Args:
            image_path: Ruta al crop
            text_description: Contexto textual rico
            fusion_strategy: 
                - "average": (img_emb + text_emb) / 2 â†’ 1024d âœ… RECOMENDADO
                - "weighted": Î±*img + Î²*text â†’ 1024d
                - "concat": [img | text] â†’ 2048d (NO recomendado)
        
        Returns:
            Embedding normalizado (1024,) float32
        """
        image = Image.open(image_path).convert('RGB')
        W, H = image.size
        
        # Decidir si usar multi-patch
        if self.use_multipatch and (W > self.patch_size or H > self.patch_size):
            return self._generate_multipatch_embedding(
                image, text_description, fusion_strategy
            )
        else:
            return self._generate_single_embedding(
                image, text_description, fusion_strategy
            )
    
    def _generate_single_embedding(
        self,
        image: Image.Image,
        text_description: str,
        fusion_strategy: str
    ) -> np.ndarray:
        """Genera embedding de imagen Ãºnica (crop pequeÃ±o)"""
        
        # Procesamiento conjunto
        inputs = self.processor(
            text=[text_description],
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # Mover a device
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            
            # Embeddings alineados nativamente en espacio compartido
            image_emb = outputs.image_embeds  # [1, 1024]
            text_emb = outputs.text_embeds    # [1, 1024]
        
        # FusiÃ³n
        if fusion_strategy == "average":
            # Promedio simple â†’ Balance perfecto
            combined = (image_emb + text_emb) / 2
        
        elif fusion_strategy == "weighted":
            # FusiÃ³n ponderada
            Î± = 0.5  # peso imagen
            Î² = 0.5  # peso texto
            combined = Î± * image_emb + Î² * text_emb
        
        elif fusion_strategy == "concat":
            # ConcatenaciÃ³n â†’ 2048d (NO recomendado)
            combined = torch.cat([image_emb, text_emb], dim=1)
        
        else:
            raise ValueError(f"Unknown fusion strategy: {fusion_strategy}")
        
        # Normalizar L2
        combined = combined / combined.norm(dim=-1, keepdim=True)
        
        return combined.cpu().numpy().flatten().astype(np.float32)
    
    def _generate_multipatch_embedding(
        self,
        image: Image.Image,
        text_description: str,
        fusion_strategy: str
    ) -> np.ndarray:
        """
        Genera embedding multi-patch para preservar detalles
        
        Estrategia:
        1. Embedding global (imagen completa con texto)
        2. Embeddings de patches (solo visual)
        3. FusiÃ³n: 60% global + 40% patches
        """
        W, H = image.size
        
        # 1. Embedding global (imagen + texto)
        global_emb = self._generate_single_embedding(
            image, text_description, fusion_strategy
        )
        
        # 2. Generar patches con sliding window
        patches = []
        patch_coords = []
        
        for y in range(0, max(H - self.patch_size + 1, 1), self.patch_stride):
            for x in range(0, max(W - self.patch_size + 1, 1), self.patch_stride):
                # Extraer patch
                patch = image.crop((
                    x, y, 
                    min(x + self.patch_size, W), 
                    min(y + self.patch_size, H)
                ))
                
                # Si patch es mÃ¡s pequeÃ±o que patch_size, hacer resize
                if patch.size != (self.patch_size, self.patch_size):
                    patch = patch.resize(
                        (self.patch_size, self.patch_size),
                        Image.LANCZOS
                    )
                
                patches.append(patch)
                patch_coords.append((x, y))
        
        # Si no hay patches (imagen muy pequeÃ±a), retornar solo global
        if not patches:
            return global_emb
        
        # 3. Embeddings de patches (solo visual, sin texto)
        patch_embeddings = []
        
        for patch in patches:
            inputs = self.processor(images=patch, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                patch_embeddings.append(outputs.image_embeds)
        
        # 4. Fusionar patches (promedio)
        patch_tensor = torch.cat(patch_embeddings, dim=0)  # [N_patches, 1024]
        fused_patches = torch.mean(patch_tensor, dim=0, keepdim=True)  # [1, 1024]
        
        # 5. Combinar global + patches
        # Global tiene contexto textual â†’ mÃ¡s peso
        # Patches tienen detalles visuales â†’ menos peso
        global_tensor = torch.from_numpy(global_emb).unsqueeze(0).to(self.device)
        combined = 0.6 * global_tensor + 0.4 * fused_patches
        
        # Normalizar
        combined = combined / combined.norm(dim=-1, keepdim=True)
        
        return combined.cpu().numpy().flatten().astype(np.float32)
    
    def generate_batch_embeddings(
        self,
        image_paths: List[Path],
        text_descriptions: List[str],
        batch_size: int = 8,
        fusion_strategy: str = "average",
        show_progress: bool = True
    ) -> Tuple[np.ndarray, List[Dict]]:
        """
        Genera embeddings en batch
        
        Returns:
            embeddings: np.ndarray (N, 1024)
            debug_info: List[dict] con info de cada embedding
        """
        assert len(image_paths) == len(text_descriptions), \
            "image_paths y text_descriptions deben tener mismo tamaÃ±o"
        
        n_samples = len(image_paths)
        all_embeddings = []
        debug_info = []
        
        if show_progress:
            from tqdm import tqdm
            pbar = tqdm(total=n_samples, desc="Generating MetaCLIP embeddings")
        
        for i in range(0, n_samples, batch_size):
            batch_end = min(i + batch_size, n_samples)
            batch_paths = image_paths[i:batch_end]
            batch_texts = text_descriptions[i:batch_end]
            
            for img_path, text in zip(batch_paths, batch_texts):
                try:
                    emb = self.generate_unified_embedding(
                        image_path=img_path,
                        text_description=text,
                        fusion_strategy=fusion_strategy
                    )
                    
                    all_embeddings.append(emb)
                    
                    debug_info.append({
                        'image_path': str(img_path),
                        'text_description': text,
                        'embedding_norm': float(np.linalg.norm(emb)),
                        'fusion_strategy': fusion_strategy,
                        'multi_patch_used': self.use_multipatch
                    })
                
                except Exception as e:
                    print(f"\nâŒ Error en {img_path.name}: {e}")
                    # Embedding cero si falla
                    all_embeddings.append(
                        np.zeros(self.embedding_dim, dtype=np.float32)
                    )
                    debug_info.append({
                        'error': str(e),
                        'image_path': str(img_path)
                    })
                
                if show_progress:
                    pbar.update(1)
        
        if show_progress:
            pbar.close()
        
        embeddings = np.vstack(all_embeddings)
        
        print(f"\n{'='*70}")
        print(f"âœ… BATCH EMBEDDINGS GENERADOS")
        print(f"{'='*70}")
        print(f"Shape: {embeddings.shape}")
        print(f"Dtype: {embeddings.dtype}")
        print(f"Norma promedio: {np.linalg.norm(embeddings, axis=1).mean():.4f}")
        print(f"{'='*70}\n")
        
        return embeddings, debug_info
    
    def get_model_info(self) -> Dict:
        """Retorna informaciÃ³n del modelo"""
        return {
            'model_type': 'metaclip-2',
            'model_name': self.model.config.name_or_path,
            'embedding_dim': self.embedding_dim,
            'total_params': sum(p.numel() for p in self.model.parameters()),
            'device': str(self.device),
            'supports_multipatch': self.use_multipatch,
            'patch_size': self.patch_size if self.use_multipatch else None,
            'patch_stride': self.patch_stride if self.use_multipatch else None
        }
```

#### Comparativa de Estrategias de FusiÃ³n

| Estrategia | DimensiÃ³n | Pros | Contras | Uso |
|------------|-----------|------|---------|-----|
| **average** | 1024d | Balance perfecto, eficiente | - | âœ… **RECOMENDADO** |
| **weighted** | 1024d | Ajustable (Î±, Î²) | Requiere tuning | Alternativa |
| **concat** | 2048d | Preserva todo | Pesado para FAISS | âŒ NO usar |

**RecomendaciÃ³n**: Usar `fusion_strategy="average"` â†’ **1024d**

---

### 6. Unified FAISS Index Builder ğŸ—„ï¸

**UbicaciÃ³n**: `src/core/vector_store/unified_faiss_builder.py`

**FunciÃ³n**: Construir Ã­ndice FAISS Ãºnico con todos los crops (damaged + clean).

**ConfiguraciÃ³n Optimizada**:

```python
# Para ~2,500 vectores de 1024 dims: IndexHNSWFlat
index = faiss.IndexHNSWFlat(1024, 32)
index.hnsw.efConstruction = 200
index.hnsw.efSearch = 64
```

**ParÃ¡metros HNSW**:
- **M**: 32 (conectividad del grafo)
- **efConstruction**: 200 (calidad durante construcciÃ³n)
- **efSearch**: 64 (calidad durante bÃºsqueda)

**TamaÃ±o Estimado**:
- ~2,500 vectores Ã— 1024 dims Ã— 4 bytes â‰ˆ **10 MB**
- Con overhead HNSW: **~15-18 MB** âœ…

**Ventaja vs. Arquitectura Anterior**:
- Antes: 1408 dims â†’ ~20-25 MB
- Ahora: 1024 dims â†’ **~15-18 MB** (28% reducciÃ³n)

---

### 7. Unified Retriever con Filtros ğŸ”

**UbicaciÃ³n**: `src/core/rag/retriever_unified.py`

**FunciÃ³n**: BÃºsqueda semÃ¡ntica con filtros pre-FAISS.

#### Filtros Soportados

| Filtro | Tipo | DescripciÃ³n |
|--------|------|-------------|
| `vehicle_zone` | str o List[str] | Zona(s) del vehÃ­culo (1-10) |
| `has_damage` | bool | Con daÃ±o (True) o sin daÃ±o (False) |
| `damage_type` | str o List[str] | Tipo(s) de daÃ±o especÃ­fico(s) |

#### Ejemplo de Uso

```python
# Buscar solo zona 4 con daÃ±o
results = retriever.search(
    query_embedding=query_emb,
    k=5,
    filters={
        'vehicle_zone': '4',
        'has_damage': True
    }
)

# Buscar imÃ¡genes limpias en zonas frontales
results = retriever.search(
    query_embedding=query_emb,
    k=10,
    filters={
        'vehicle_zone': ['1', '2', '3'],
        'has_damage': False
    }
)

# Buscar araÃ±azos superficiales
results = retriever.search(
    query_embedding=query_emb,
    k=5,
    filters={
        'damage_type': 'surface_scratch'
    }
)
```

---

## ğŸš€ Pipeline de ImplementaciÃ³n

### FASE 1: GeneraciÃ³n de Crops Optimizados ğŸ“¸

**Script**: `scripts/phase1_generate_crops_optimized.py`  
**DuraciÃ³n Estimada**: 15-20 minutos

#### Proceso

```
Dataset Split 2 (552 imÃ¡genes)
           â†“
    Â¿Tipo de imagen?
     â†™         â†˜
   ko          ok
(276)         (276)
    â†“            â†“
Clustered    Vehicle Detector
  Crop       + Grid Crop
Generator      Generator
(sin padding)  (448Ã—448)
    â†“            â†“
~850 crops   ~1,500-2,000 crops
    â†“            â†“
damaged/     clean/
```

#### ImplementaciÃ³n

```python
#!/usr/bin/env python3
# scripts/phase1_generate_crops_optimized.py

from pathlib import Path
from src.core.preprocessing.vehicle_detector import VehicleDetector
from src.core.preprocessing.grid_crop_generator import GridCropGenerator
from src.core.preprocessing.clustered_crop_generator_optimized import ClusteredCropGeneratorOptimized
import json
from tqdm import tqdm

def main():
    # ConfiguraciÃ³n
    dataset_dir = Path("data/raw/dataset_split_2")
    output_dir = Path("data/processed/crops/balanced_optimized")
    
    output_dir_damaged = output_dir / "damaged"
    output_dir_clean = output_dir / "clean"
    
    output_dir_damaged.mkdir(parents=True, exist_ok=True)
    output_dir_clean.mkdir(parents=True, exist_ok=True)
    
    # Inicializar componentes
    print("ğŸ”§ Inicializando componentes...")
    vehicle_detector = VehicleDetector()
    grid_generator = GridCropGenerator(
        crop_size=448,
        overlap=0.25,
        min_vehicle_ratio=0.70
    )
    cluster_generator = ClusteredCropGeneratorOptimized(
        target_size=448,
        use_padding=False  # â† SIN PADDING
    )
    
    # Escanear dataset
    all_images = list(dataset_dir.glob("*_imageDANO_original.jpg"))
    
    ko_images = [img for img in all_images if "_ko_" in img.name]
    ok_images = [img for img in all_images if "_ok_" in img.name]
    
    print(f"\nğŸ“Š Dataset:")
    print(f"   - ImÃ¡genes CON daÃ±o: {len(ko_images)}")
    print(f"   - ImÃ¡genes SIN daÃ±o: {len(ok_images)}")
    print(f"   - Total: {len(all_images)}\n")
    
    metadata_list = []
    
    # Procesar imÃ¡genes CON daÃ±o
    print("ğŸ”§ Procesando imÃ¡genes CON daÃ±o (clustered crops)...")
    for img_path in tqdm(ko_images, desc="Crops damaged"):
        # Buscar JSON de segmentaciÃ³n
        json_path = img_path.parent / img_path.name.replace(
            '_imageDANO_original.jpg',
            '_labelDANO_modificado.json'
        )
        
        if not json_path.exists():
            continue
        
        # Generar crops
        crops = cluster_generator.generate_crops(
            image_path=img_path,
            json_path=json_path
        )
        
        # Guardar crops
        for i, crop_data in enumerate(crops):
            crop_id = img_path.stem + f"_cluster_{i:03d}"
            crop_path = output_dir_damaged / f"{crop_id}.jpg"
            
            cv2.imwrite(str(crop_path), crop_data['crop'])
            
            # Metadata preliminar
            metadata_list.append({
                'crop_id': crop_id,
                'crop_path': str(crop_path),
                'source_image': img_path.name,
                'has_damage': True,
                'vehicle_zone': extract_zone_from_filename(img_path.name),
                'crop_type': 'clustered',
                'crop_size': crop_data['crop'].shape[:2],
                'defects': crop_data.get('defects', [])
            })
    
    # Procesar imÃ¡genes SIN daÃ±o
    print("\nğŸ”§ Procesando imÃ¡genes SIN daÃ±o (grid crops)...")
    for img_path in tqdm(ok_images, desc="Crops clean"):
        # Detectar vehÃ­culo
        detection = vehicle_detector.detect(img_path)
        
        if detection is None:
            continue
        
        # Generar grid crops
        crops = grid_generator.generate_crops(
            image_path=img_path,
            vehicle_bbox=detection['bbox']
        )
        
        # Guardar crops
        for i, crop_data in enumerate(crops):
            crop_id = img_path.stem + f"_grid_{crop_data['grid_x']}_{crop_data['grid_y']}"
            crop_path = output_dir_clean / f"{crop_id}.jpg"
            
            cv2.imwrite(str(crop_path), crop_data['crop'])
            
            # Metadata preliminar
            metadata_list.append({
                'crop_id': crop_id,
                'crop_path': str(crop_path),
                'source_image': img_path.name,
                'has_damage': False,
                'vehicle_zone': extract_zone_from_filename(img_path.name),
                'crop_type': 'grid',
                'crop_size': crop_data['crop'].shape[:2],
                'grid_position': [crop_data['grid_x'], crop_data['grid_y']]
            })
    
    # Guardar metadata preliminar
    metadata_path = Path("data/processed/metadata/balanced_crops_preliminary.json")
    metadata_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(metadata_path, 'w') as f:
        json.dump(metadata_list, f, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… FASE 1 COMPLETADA")
    print(f"{'='*70}")
    print(f"Crops damaged: {len([m for m in metadata_list if m['has_damage']])}")
    print(f"Crops clean: {len([m for m in metadata_list if not m['has_damage']])}")
    print(f"Total crops: {len(metadata_list)}")
    print(f"Metadata guardada: {metadata_path}")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
```

#### Output

```
data/processed/crops/balanced_optimized/
â”œâ”€â”€ damaged/
â”‚   â”œâ”€â”€ zona1_ko_..._cluster_000.jpg
â”‚   â”œâ”€â”€ zona1_ko_..._cluster_001.jpg
â”‚   â””â”€â”€ ... (~850 crops, sin padding)
â””â”€â”€ clean/
    â”œâ”€â”€ zona1_ok_..._grid_0_0.jpg
    â”œâ”€â”€ zona1_ok_..._grid_0_1.jpg
    â””â”€â”€ ... (~1,500-2,000 crops)

data/processed/metadata/
â””â”€â”€ balanced_crops_preliminary.json
```

#### MÃ©tricas de Ã‰xito

- âœ… ~850 crops damaged (sin padding artificial)
- âœ… ~1,500-2,000 crops clean
- âœ… Todos los crops preservan informaciÃ³n original
- âœ… Crops clean con >70% Ã¡rea del vehÃ­culo

---

### FASE 2: GeneraciÃ³n de Contextos Enriquecidos ğŸ“

**Script**: `scripts/phase2_generate_contexts.py`  
**DuraciÃ³n Estimada**: 20-30 minutos

#### Proceso

```
Metadata Preliminar
        â†“
  Â¿Tiene daÃ±o?
   â†™         â†˜
 SÃ          NO
  â†“            â†“
Cargar JSON  Contexto
segmentaciÃ³n  limpio
  â†“            â†“
Extraer tipos   â†“
+ relaciones    â†“
espaciales      â†“
  â†“            â†“
  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â†“
DamageContextualizer
        â†“
text_description
        â†“
Metadata Enriquecida
```

#### ImplementaciÃ³n

```python
#!/usr/bin/env python3
# scripts/phase2_generate_contexts.py

from pathlib import Path
from src.core.embeddings.damage_contextualizer import DamageContextualizer
import json
from tqdm import tqdm

def main():
    # Cargar metadata preliminar
    metadata_path = Path("data/processed/metadata/balanced_crops_preliminary.json")
    
    with open(metadata_path) as f:
        metadata_list = json.load(f)
    
    print(f"ğŸ“Š Metadata cargada: {len(metadata_list)} crops\n")
    
    # Inicializar contextualizador
    contextualizer = DamageContextualizer()
    
    # Enriquecer metadata
    print("ğŸ“ Generando contextos textuales...")
    
    for meta in tqdm(metadata_list, desc="Generating contexts"):
        if meta['has_damage']:
            # Cargar JSON de segmentaciÃ³n
            source_img = meta['source_image']
            json_path = Path("data/raw/dataset_split_2") / source_img.replace(
                '_imageDANO_original.jpg',
                '_labelDANO_modificado.json'
            )
            
            with open(json_path) as f:
                seg_data = json.load(f)
            
            # Extraer info de daÃ±os
            meta['damage_types'] = [
                shape['label'] for shape in seg_data['shapes']
                if shape['label'] != '9'
            ]
            
            # Generar contexto
            text_desc = contextualizer.build_damage_context(meta)
        
        else:
            # Generar contexto limpio
            text_desc = contextualizer.build_clean_context(meta)
        
        # Actualizar metadata
        meta['text_description'] = text_desc
    
    # Guardar metadata enriquecida
    output_path = Path("data/processed/metadata/balanced_crops_enriched.json")
    
    with open(output_path, 'w') as f:
        json.dump(metadata_list, f, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… FASE 2 COMPLETADA")
    print(f"{'='*70}")
    print(f"Metadata enriquecida guardada: {output_path}")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
```

#### Output

```
data/processed/metadata/
â””â”€â”€ balanced_crops_enriched.json  # Con text_description
```

#### MÃ©tricas de Ã‰xito

- âœ… 100% crops con `text_description`
- âœ… Longitud promedio: 150-180 caracteres
- âœ… Contextos coherentes y descriptivos

---

### FASE 3: GeneraciÃ³n de Embeddings MetaCLIP ğŸ§ 

**Script**: `scripts/phase3_generate_metaclip_embeddings.py`  
**DuraciÃ³n Estimada**: 25-30 minutos

#### Proceso

```
Metadata Enriquecida
        â†“
MetaCLIP 2 Embedder
        â†“
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â†“       â†“
Visual   Text
(aligned natively)
    â†“       â†“
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜
        â†“
Fusion (average)
        â†“
NormalizaciÃ³n L2
        â†“
Embeddings 1024d
```

#### ImplementaciÃ³n

```python
#!/usr/bin/env python3
# scripts/phase3_generate_metaclip_embeddings.py

from pathlib import Path
from src.core.embeddings.metaclip_embedder_unified import MetaCLIPUnifiedEmbedder
import json
import numpy as np
from datetime import datetime

def main():
    # Cargar metadata enriquecida
    metadata_path = Path("data/processed/metadata/balanced_crops_enriched.json")
    
    with open(metadata_path) as f:
        metadata_list = json.load(f)
    
    print(f"ğŸ“Š Metadata cargada: {len(metadata_list)} crops\n")
    
    # Inicializar embedder
    embedder = MetaCLIPUnifiedEmbedder(
        model_name="facebook/metaclip-h14-fullcc2.5b",
        use_multipatch=True,
        patch_size=336,
        patch_stride=224
    )
    
    # Preparar inputs
    image_paths = [Path(m['crop_path']) for m in metadata_list]
    text_descriptions = [m['text_description'] for m in metadata_list]
    
    # Generar embeddings
    embeddings, debug_info = embedder.generate_batch_embeddings(
        image_paths=image_paths,
        text_descriptions=text_descriptions,
        batch_size=8,
        fusion_strategy="average",
        show_progress=True
    )
    
    # Enriquecer metadata final
    for i, meta in enumerate(metadata_list):
        meta['embedding_index'] = i
        meta['embedding_model'] = 'metaclip-h14-fullcc2.5b'
        meta['embedding_dim'] = int(embeddings.shape[1])
        meta['embedding_norm'] = float(np.linalg.norm(embeddings[i]))
        meta['fusion_strategy'] = 'average'
    
    # Guardar
    output_dir = Path("data/processed/embeddings/metaclip_balanced")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Embeddings
    embeddings_path = output_dir / "embeddings_metaclip.npy"
    np.save(embeddings_path, embeddings)
    print(f"ğŸ’¾ Embeddings guardados: {embeddings_path}")
    
    # Metadata final
    metadata_final_path = output_dir / "metadata_final.json"
    with open(metadata_final_path, 'w') as f:
        json.dump(metadata_list, f, indent=2)
    print(f"ğŸ’¾ Metadata final guardada: {metadata_final_path}")
    
    # Info del proceso
    process_info = {
        'timestamp': datetime.now().isoformat(),
        'model': embedder.get_model_info(),
        'dataset': {
            'total_crops': len(metadata_list),
            'damage_crops': sum(1 for m in metadata_list if m['has_damage']),
            'clean_crops': sum(1 for m in metadata_list if not m['has_damage'])
        },
        'embeddings': {
            'shape': list(embeddings.shape),
            'dtype': str(embeddings.dtype),
            'norm_mean': float(np.linalg.norm(embeddings, axis=1).mean()),
            'norm_std': float(np.linalg.norm(embeddings, axis=1).std())
        }
    }
    
    info_path = output_dir / "generation_info.json"
    with open(info_path, 'w') as f:
        json.dump(process_info, f, indent=2)
    print(f"ğŸ’¾ Info del proceso: {info_path}")
    
    print(f"\n{'='*70}")
    print(f"âœ… FASE 3 COMPLETADA")
    print(f"{'='*70}")
    print(f"Embeddings shape: {embeddings.shape}")
    print(f"Embedding dim: {embeddings.shape[1]}")
    print(f"Norma promedio: {np.linalg.norm(embeddings, axis=1).mean():.4f}")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
```

#### Output

```
data/processed/embeddings/metaclip_balanced/
â”œâ”€â”€ embeddings_metaclip.npy      # (2500, 1024) float32
â”œâ”€â”€ metadata_final.json           # Con embedding_index
â””â”€â”€ generation_info.json          # Info del proceso
```

#### EstadÃ­sticas Esperadas

```
Shape: (2500, 1024)
Dtype: float32
Norma promedio: 1.0000 (Â±0.0001)
Tiempo total: ~25-30 minutos
Tiempo/crop: ~0.6-0.7s
```

#### MÃ©tricas de Ã‰xito

- âœ… Embeddings shape: (N, 1024)
- âœ… Normas promedio: ~1.0
- âœ… Sin NaN o Inf
- âœ… 28% mÃ¡s eficiente que 1408d

---

### FASE 4: ConstrucciÃ³n Ãndice FAISS ğŸ—„ï¸

**Script**: `scripts/phase4_build_unified_faiss_index.py`  
**DuraciÃ³n Estimada**: 1-2 minutos

#### ImplementaciÃ³n

```python
#!/usr/bin/env python3
# scripts/phase4_build_unified_faiss_index.py

from pathlib import Path
import json
import numpy as np
import faiss
import pickle

def main():
    # Cargar embeddings y metadata
    embeddings_path = Path("data/processed/embeddings/metaclip_balanced/embeddings_metaclip.npy")
    metadata_path = Path("data/processed/embeddings/metaclip_balanced/metadata_final.json")
    
    embeddings = np.load(embeddings_path).astype('float32')
    
    with open(metadata_path) as f:
        metadata = json.load(f)
    
    n_vectors, dim = embeddings.shape
    
    print(f"\n{'='*70}")
    print(f"ğŸ—ï¸  CONSTRUCCIÃ“N ÃNDICE FAISS")
    print(f"{'='*70}")
    print(f"Embeddings: {embeddings.shape}")
    print(f"DimensiÃ³n: {dim}")
    print(f"Metadata: {len(metadata)} entries\n")
    
    # EstadÃ­sticas
    damage_count = sum(1 for m in metadata if m['has_damage'])
    clean_count = len(metadata) - damage_count
    
    print(f"ğŸ“Š Dataset:")
    print(f"   - CON daÃ±o: {damage_count} ({damage_count/n_vectors*100:.1f}%)")
    print(f"   - SIN daÃ±o: {clean_count} ({clean_count/n_vectors*100:.1f}%)\n")
    
    # Construir Ã­ndice HNSW
    M = 32
    index = faiss.IndexHNSWFlat(dim, M)
    index.hnsw.efConstruction = 200
    index.hnsw.efSearch = 64
    
    print(f"ğŸ”§ Ãndice: IndexHNSWFlat")
    print(f"   - M: {M}")
    print(f"   - efConstruction: 200")
    print(f"   - efSearch: 64\n")
    
    # AÃ±adir vectores
    index.add(embeddings)
    print(f"âœ… Vectores aÃ±adidos: {index.ntotal}\n")
    
    # ValidaciÃ³n
    distances, indices = index.search(embeddings[0:1], k=5)
    
    print(f"ğŸ” ValidaciÃ³n (self-similarity):")
    print(f"   - Top-1 index: {indices[0][0]} (esperado: 0)")
    print(f"   - Top-1 distance: {distances[0][0]:.4f} (esperado: ~0.0)")
    
    assert indices[0][0] == 0, "Error: primer resultado no es Ã©l mismo"
    assert distances[0][0] < 0.01, "Error: distancia a sÃ­ mismo > 0.01"
    print(f"   âœ… ValidaciÃ³n exitosa\n")
    
    # Guardar
    output_dir = Path("outputs/vector_indices/metaclip_balanced")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Ãndice FAISS
    index_path = output_dir / "index_metaclip_balanced.faiss"
    faiss.write_index(index, str(index_path))
    print(f"ğŸ’¾ Ãndice FAISS: {index_path}")
    
    # Metadata (pickle)
    metadata_pkl_path = output_dir / "metadata_balanced.pkl"
    with open(metadata_pkl_path, 'wb') as f:
        pickle.dump(metadata, f)
    print(f"ğŸ’¾ Metadata (pickle): {metadata_pkl_path}")
    
    # ConfiguraciÃ³n
    config = {
        'index_type': 'IndexHNSWFlat',
        'embedding_model': 'metaclip-h14-fullcc2.5b',
        'embedding_dim': dim,
        'fusion_strategy': 'average',
        'n_vectors': n_vectors,
        'M': M,
        'efConstruction': 200,
        'efSearch': 64,
        'damage_crops': damage_count,
        'clean_crops': clean_count,
        'index_size_mb': index_path.stat().st_size / (1024 * 1024)
    }
    
    config_path = output_dir / "config.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"ğŸ’¾ Config: {config_path}")
    
    print(f"\n{'='*70}")
    print(f"âœ… FASE 4 COMPLETADA")
    print(f"{'='*70}")
    print(f"Ãndice: {index.ntotal} vectores")
    print(f"TamaÃ±o: {config['index_size_mb']:.2f} MB")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
```

#### Output

```
outputs/vector_indices/metaclip_balanced/
â”œâ”€â”€ index_metaclip_balanced.faiss  # ~15-18 MB
â”œâ”€â”€ metadata_balanced.pkl           # ~15-20 MB
â””â”€â”€ config.json                     # ConfiguraciÃ³n
```

#### MÃ©tricas de Ã‰xito

- âœ… Ãndice construido: 2,500 vectores
- âœ… TamaÃ±o: ~15-18 MB (28% mÃ¡s eficiente que 1408d)
- âœ… ValidaciÃ³n exitosa

---

### FASE 5: ValidaciÃ³n y Testing âœ…

**Script**: `scripts/phase5_validate_retrieval.py`  
**DuraciÃ³n Estimada**: 5-10 minutos

#### Tests Principales

1. **Filtros bÃ¡sicos** (zona, has_damage)
2. **Cobertura de retrieval** (damaged â†’ damaged, clean â†’ clean)
3. **Calidad de contextos** (text_description coherente)
4. **VisualizaciÃ³n manual** (top-K resultados)

#### ImplementaciÃ³n

```python
#!/usr/bin/env python3
# scripts/phase5_validate_retrieval.py

from pathlib import Path
from src.core.rag.retriever_unified import MetaCLIPUnifiedRetriever
import numpy as np
import random

def main():
    # Cargar retriever
    index_path = Path("outputs/vector_indices/metaclip_balanced/index_metaclip_balanced.faiss")
    metadata_path = Path("outputs/vector_indices/metaclip_balanced/metadata_balanced.pkl")
    
    retriever = MetaCLIPUnifiedRetriever(
        index_path=str(index_path),
        metadata_path=str(metadata_path)
    )
    
    # Cargar embeddings para testing
    embeddings = np.load("data/processed/embeddings/metaclip_balanced/embeddings_metaclip.npy")
    
    print(f"\n{'='*70}")
    print(f"ğŸ§ª VALIDACIÃ“N DEL RETRIEVER")
    print(f"{'='*70}\n")
    
    # Test 1: Filtros bÃ¡sicos
    print("ğŸ“‹ TEST 1: Filtros BÃ¡sicos\n")
    
    # Test 1.1: Zona 4 con daÃ±o
    test_emb = embeddings[0]
    results = retriever.search(
        query_embedding=test_emb,
        k=5,
        filters={'vehicle_zone': '4', 'has_damage': True}
    )
    
    print("Test 1.1: Zona 4 con daÃ±o")
    for r in results:
        assert r['vehicle_zone'] == '4'
        assert r['has_damage'] == True
        print(f"   âœ… {r['crop_id']} - Zone: {r['zone_description']}")
    
    # Test 1.2: Zonas frontales sin daÃ±o
    results = retriever.search(
        query_embedding=test_emb,
        k=5,
        filters={'vehicle_zone': ['1', '2', '3'], 'has_damage': False}
    )
    
    print("\nTest 1.2: Zonas frontales sin daÃ±o")
    for r in results:
        assert r['vehicle_zone'] in ['1', '2', '3']
        assert r['has_damage'] == False
        print(f"   âœ… {r['crop_id']} - Zone: {r['zone_description']}")
    
    # Test 2: Cobertura
    print(f"\n{'='*70}")
    print("ğŸ“‹ TEST 2: Cobertura de Retrieval\n")
    
    # Query damaged â†’ recuperar damaged
    damaged_indices = [i for i, m in enumerate(retriever.metadata) if m['has_damage']]
    sample_damaged = random.sample(damaged_indices, min(10, len(damaged_indices)))
    
    damage_coverage = []
    for idx in sample_damaged:
        query_emb = embeddings[idx]
        results = retriever.search(query_emb, k=5)
        
        damaged_count = sum(1 for r in results if r['has_damage'])
        damage_coverage.append(damaged_count / 5)
    
    avg_damage_coverage = np.mean(damage_coverage)
    print(f"Query damaged â†’ Damaged retrieved: {avg_damage_coverage*100:.1f}%")
    assert avg_damage_coverage >= 0.6, "Cobertura de damaged < 60%"
    print(f"   âœ… Cobertura adecuada (â‰¥60%)")
    
    # Query clean â†’ recuperar clean
    clean_indices = [i for i, m in enumerate(retriever.metadata) if not m['has_damage']]
    sample_clean = random.sample(clean_indices, min(10, len(clean_indices)))
    
    clean_coverage = []
    for idx in sample_clean:
        query_emb = embeddings[idx]
        results = retriever.search(query_emb, k=5)
        
        clean_count = sum(1 for r in results if not r['has_damage'])
        clean_coverage.append(clean_count / 5)
    
    avg_clean_coverage = np.mean(clean_coverage)
    print(f"Query clean â†’ Clean retrieved: {avg_clean_coverage*100:.1f}%")
    assert avg_clean_coverage >= 0.6, "Cobertura de clean < 60%"
    print(f"   âœ… Cobertura adecuada (â‰¥60%)")
    
    print(f"\n{'='*70}")
    print(f"âœ… TODAS LAS VALIDACIONES EXITOSAS")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
```

#### MÃ©tricas de Ã‰xito

- âœ… Filtros funcionan correctamente
- âœ… Cobertura damaged â‰¥ 60%
- âœ… Cobertura clean â‰¥ 60%
- âœ… Contextos coherentes

---

## ğŸ“ Estructura de Archivos

```
RAG-multimodal/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ dataset_split_2/                    # Dataset original (552 imÃ¡genes)
â”‚   â”‚
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ crops/
â”‚       â”‚   â””â”€â”€ balanced_optimized/
â”‚       â”‚       â”œâ”€â”€ damaged/                    # ~850 crops (sin padding)
â”‚       â”‚       â””â”€â”€ clean/                      # ~1,500-2,000 crops
â”‚       â”‚
â”‚       â”œâ”€â”€ metadata/
â”‚       â”‚   â”œâ”€â”€ balanced_crops_preliminary.json
â”‚       â”‚   â””â”€â”€ balanced_crops_enriched.json
â”‚       â”‚
â”‚       â””â”€â”€ embeddings/
â”‚           â””â”€â”€ metaclip_balanced/
â”‚               â”œâ”€â”€ embeddings_metaclip.npy     # (2500, 1024)
â”‚               â”œâ”€â”€ metadata_final.json
â”‚               â””â”€â”€ generation_info.json
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ preprocessing/
â”‚       â”‚   â”œâ”€â”€ vehicle_detector.py
â”‚       â”‚   â”œâ”€â”€ grid_crop_generator.py
â”‚       â”‚   â””â”€â”€ clustered_crop_generator_optimized.py  # SIN padding
â”‚       â”‚
â”‚       â”œâ”€â”€ embeddings/
â”‚       â”‚   â”œâ”€â”€ damage_contextualizer.py
â”‚       â”‚   â””â”€â”€ metaclip_embedder_unified.py   # âœ¨ NUEVO
â”‚       â”‚
â”‚       â”œâ”€â”€ vector_store/
â”‚       â”‚   â””â”€â”€ unified_faiss_builder.py
â”‚       â”‚
â”‚       â””â”€â”€ rag/
â”‚           â””â”€â”€ retriever_unified.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ phase1_generate_crops_optimized.py     # âœ¨ ACTUALIZADO
â”‚   â”œâ”€â”€ phase2_generate_contexts.py
â”‚   â”œâ”€â”€ phase3_generate_metaclip_embeddings.py # âœ¨ NUEVO
â”‚   â”œâ”€â”€ phase4_build_unified_faiss_index.py    # âœ¨ ACTUALIZADO
â”‚   â””â”€â”€ phase5_validate_retrieval.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ vector_indices/
â”‚       â””â”€â”€ metaclip_balanced/
â”‚           â”œâ”€â”€ index_metaclip_balanced.faiss  # ~15-18 MB
â”‚           â”œâ”€â”€ metadata_balanced.pkl
â”‚           â””â”€â”€ config.json
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ crop_strategy_config.yaml
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ BALANCED_DATASET_ARCHITECTURE_METACLIP2.md  # ğŸ“„ ESTE DOCUMENTO
â”‚
â””â”€â”€ requirements.txt
```

---

## ğŸ“Š Esquema de Metadata

### Metadata Final (JSON)

```json
{
  "crop_id": "zona1_ko_2_3_1554817134014_zona_4_cluster_003",
  "crop_path": "data/processed/crops/balanced_optimized/damaged/zona1_ko_..._cluster_003.jpg",
  "source_image": "zona1_ko_2_3_1554817134014_zona_4_imageDANO_original.jpg",
  
  "has_damage": true,
  "vehicle_zone": "4",
  "zone_description": "rear_left_quarter",
  "zone_area": "posterior",
  
  "specific_part": "Rear left corner panel near bumper junction",
  "text_description": "Vehicle zone: rear_left_quarter (posterior area). Affected part: Rear left corner panel near bumper junction. Damage types: surface scratch (minor surface abrasion, 2 instances), dent (metal deformation, 1 instance). Spatial pattern: defects clustered together, suggesting single impact event.",
  
  "damage_types": ["surface_scratch", "dent"],
  "damage_count": 3,
  
  "crop_type": "clustered",
  "crop_size": [380, 420],
  
  "embedding_index": 42,
  "embedding_model": "metaclip-h14-fullcc2.5b",
  "embedding_dim": 1024,
  "embedding_norm": 1.0000,
  "fusion_strategy": "average"
}
```

---

## âš™ï¸ ConfiguraciÃ³n y ParÃ¡metros

### ParÃ¡metros Clave

| ParÃ¡metro | Valor | JustificaciÃ³n |
|-----------|-------|---------------|
| **Crop Size** | 448Ã—448 | Balance calidad/eficiencia |
| **MetaCLIP Patch** | 336Ã—336 | ResoluciÃ³n nativa H/14 |
| **Multi-patch Stride** | 224 | 33% overlap |
| **Grid Overlap** | 25% | Cobertura completa |
| **Min Vehicle Ratio** | 70% | Minimizar fondo |
| **Fusion Strategy** | average | Balance perfecto |
| **Embedding Dim** | 1024 | Espacio unificado |
| **FAISS Index** | IndexHNSWFlat | Ã“ptimo <10K |
| **HNSW M** | 32 | Conectividad |
| **efConstruction** | 200 | Calidad construcciÃ³n |
| **efSearch** | 64 | Calidad bÃºsqueda |

---

## ğŸ“ˆ Resultados Esperados

### Comparativa Final: DINOv3+BERT vs. MetaCLIP 2

| MÃ©trica | DINOv3+BERT (50/50) | MetaCLIP 2 (average) | Mejora |
|---------|---------------------|----------------------|--------|
| **DimensiÃ³n** | 1408d | 1024d | âœ… 27% reducciÃ³n |
| **Ãndice FAISS** | ~20-25 MB | ~15-18 MB | âœ… 28% reducciÃ³n |
| **Velocidad embedding** | ~0.8s/crop | ~0.6s/crop | âœ… 25% mÃ¡s rÃ¡pido |
| **VRAM** | ~8GB | ~6GB | âœ… 25% reducciÃ³n |
| **AlineaciÃ³n** | Manual | Nativa | âœ… Superior |
| **Retrieval esperado** | Bueno | Excelente | âœ… +10-15% |
| **Mantenimiento** | Complejo | Simple | âœ… 1 modelo |

### EstadÃ­sticas del Pipeline

| Fase | Input | Output | Tiempo |
|------|-------|--------|--------|
| Fase 1 | 552 imÃ¡genes | ~2,500 crops | 15-20 min |
| Fase 2 | 2,500 crops | Contextos | 20-30 min |
| Fase 3 | 2,500 crops | Embeddings 1024d | 25-30 min |
| Fase 4 | 2,500 embeddings | Ãndice FAISS | 1-2 min |
| Fase 5 | Ãndice | ValidaciÃ³n | 5-10 min |
| **TOTAL** | - | - | **~1.5 horas** |

---

## ğŸš€ PrÃ³ximos Pasos

### ImplementaciÃ³n Inmediata

1. âœ… **Fase 1**: Generar crops optimizados (sin padding)
2. âœ… **Fase 2**: Contextos enriquecidos
3. âœ… **Fase 3**: Embeddings MetaCLIP 2
4. âœ… **Fase 4**: Ãndice FAISS unificado
5. âœ… **Fase 5**: ValidaciÃ³n

### Mejoras Futuras

#### Corto Plazo
- â˜ Experimentar con `fusion_strategy="weighted"` (Î±/Î² ajustables)
- â˜ Agregar mÃ¡s filtros (severidad, Ã¡rea afectada)
- â˜ Implementar re-ranking con cross-attention
- â˜ A/B testing vs. arquitectura anterior

#### Medio Plazo
- â˜ Fine-tuning de MetaCLIP en dataset vehicular
- â˜ Integrar VLM ligero para `specific_part` (Florence-2, PaliGemma)
- â˜ Hard negative mining
- â˜ Explorar MetaCLIP 2 Worldwide (multilenguaje)

#### Largo Plazo
- â˜ Multi-modal fusion con attention weights dinÃ¡micos
- â˜ Integrar SAM para segmentaciÃ³n automÃ¡tica
- â˜ Sistema de active learning
- â˜ Deploy en producciÃ³n con API REST

---

## ğŸ“š Referencias

### Papers CientÃ­ficos

1. **MetaCLIP**: "Demystifying CLIP Data" (2023)
   - https://arxiv.org/abs/2309.16671

2. **MetaCLIP 2**: "A Worldwide Scaling Recipe" (2024)
   - https://arxiv.org/abs/2507.22062

3. **CLIP**: "Learning Transferable Visual Models From Natural Language Supervision" (2021)
   - https://arxiv.org/abs/2103.00020

4. **HNSW**: "Efficient and robust approximate nearest neighbor search" (2018)
   - https://arxiv.org/abs/1603.09320

5. **RAG**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (2020)
   - https://arxiv.org/abs/2005.11401

### Herramientas

- **FAISS**: https://github.com/facebookresearch/faiss
- **Transformers**: https://huggingface.co/docs/transformers
- **MetaCLIP Models**: https://huggingface.co/facebook
- **YOLOv8**: https://docs.ultralytics.com/

---

## ğŸ“ Changelog

### v4.0 (MetaCLIP 2 - Current)
- âœ¨ MigraciÃ³n completa a MetaCLIP 2
- âœ¨ Embeddings unificados 1024d (espacio compartido)
- âœ¨ Multi-patch strategy para preservar detalles
- âœ¨ Eliminado padding artificial
- âœ¨ FusiÃ³n nativa (average strategy)
- ğŸ“‰ 27% reducciÃ³n dimensionalidad
- ğŸ“‰ 28% reducciÃ³n tamaÃ±o Ã­ndice
- âš¡ 25% mÃ¡s rÃ¡pido
- ğŸ’¾ 25% menos VRAM

### v3.0 (Balanced Hybrid)
- âœ¨ Dataset balanceado 50/50
- âœ¨ Grid crops para clean
- âœ¨ Contexto enriquecido
- âœ¨ Embeddings 50/50 (DINOv3+BERT)

### v2.0 (Hybrid Fullimages)
- âœ¨ Embeddings hÃ­bridos 60/40
- âœ¨ Full images
- âœ¨ Contexto bÃ¡sico

### v1.0 (Crops Only)
- âœ¨ Crops clusterizados
- âœ¨ DINOv3 puro
- âœ¨ Ãndice bÃ¡sico

---

**Ãšltima actualizaciÃ³n**: Noviembre 2024  
**VersiÃ³n del documento**: 4.0 (MetaCLIP 2)  
**Estado**: Listo para implementaciÃ³n  
**Autor**: [Tu nombre/equipo]

---

## ğŸ¯ Resumen Ejecutivo

Este documento describe una arquitectura RAG multimodal optimizada que:

1. **Preserva informaciÃ³n mÃ¡xima** mediante crops sin padding y estrategia multi-patch
2. **Unifica visual + textual** nativamente con MetaCLIP 2 (1024d)
3. **Balancea dataset** 50/50 (damaged/clean) con contextos enriquecidos
4. **Reduce complejidad** 27% (dimensiÃ³n), 28% (Ã­ndice), 25% (VRAM)
5. **Mejora retrieval** mediante alineaciÃ³n nativa end-to-end

**Resultado esperado**: Sistema RAG de alta calidad, eficiente y escalable para detecciÃ³n de defectos vehiculares.
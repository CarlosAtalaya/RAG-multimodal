# ğŸ¯ GUÃA COMPLETA: EVALUACIÃ“N RAG CORRECTA

## âŒ Lo Que Estabas Haciendo (Incorrecto)

```
Dataset â†’ Embeddings â†’ Ãndice FAISS
                â†“
        MISMOS datos como queries
                â†“
        Recall@5 = 100% (trampa!)
```

**Problema**: Data leakage - evalÃºas con los mismos datos que usaste para entrenar.

---

## âœ… EvaluaciÃ³n RAG Correcta

### Pipeline Completo

```
Dataset Completo (2,711 imÃ¡genes)
        â†“
    SPLIT 80/20
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
   â†“                 â†“
TRAIN (80%)      TEST (20%)
2,168 imgs       543 imgs (NUNCA VISTAS)
   â†“                 â†“
Generar          Usar como
Embeddings       queries
   â†“
Ãndice FAISS
   â†“
Retrieval â† Test images (queries)
   â†“
RAG Context
   â†“
Qwen3VL genera respuesta
   â†“
Evaluar vs Ground Truth
```

---

## ğŸš€ PLAN DE ACCIÃ“N (3 Horas)

### Paso 1: Train/Test Split (15 min)

```bash
# Dividir dataset en 80% train / 20% test
python scripts/06_split_train_test.py \
    --source /home/carlos/Escritorio/Proyectos-Minsait/Mapfre/carpetas-datos/jsons_segmentacion_jsonsfinales \
    --output data/raw/train_test_split \
    --test-size 0.20 \
    --seed 42
```

**Output esperado:**
```
TRAIN: 2,168 imÃ¡genes (80%)
TEST:  543 imÃ¡genes (20%)
```

**IMPORTANTE:** 
- Train set â†’ generar embeddings e Ã­ndice
- Test set â†’ NUNCA tocar hasta la evaluaciÃ³n final

---

### Paso 2: Procesar SOLO Train Set (1.5 horas)

```bash
# 2.1 Generar crops del TRAIN set
python scripts/02_generate_clustered_crops.py \
    --dataset data/raw/train_test_split/train \
    --output data/processed/crops/train_set

# 2.2 Generar embeddings del TRAIN set
python scripts/03_generate_embeddings_dinov3.py \
    --metadata data/processed/crops/train_set/metadata/clustered_crops_metadata.json \
    --output data/processed/embeddings/train_set

# 2.3 Construir Ã­ndice FAISS del TRAIN set
python scripts/04_build_faiss_index.py \
    --embeddings data/processed/embeddings/train_set/embeddings_dinov3_vitl.npy \
    --metadata data/processed/embeddings/train_set/enriched_crops_metadata_dinov3_vitl.json \
    --output outputs/vector_indices/train_set
```

---

### Paso 3: EvaluaciÃ³n RAG End-to-End (1 hora)

```bash
# Evaluar con TEST set (imÃ¡genes NUNCA vistas)
python scripts/07_evaluate_rag_end_to_end.py \
    --test-set data/raw/train_test_split/test \
    --index outputs/vector_indices/train_set/indexhnswflat_clustered.index \
    --metadata outputs/vector_indices/train_set/metadata_clustered.pkl \
    --k 5 \
    --max-cases 50  # Primero probar con 50 imÃ¡genes
```

**Esto SÃ es una evaluaciÃ³n real:**
- âœ… ImÃ¡genes test NUNCA vistas
- âœ… Genera embeddings on-the-fly
- âœ… Retrieval en Ã­ndice del train set
- âœ… Qwen3VL genera respuesta con contexto RAG
- âœ… EvalÃºa calidad vs ground truth

---

## ğŸ“Š MÃ©tricas RAG Correctas

### Retrieval Metrics
- **Recall@k**: Â¿Se recuperan los tipos de daÃ±o correctos?
- **MRR**: Â¿En quÃ© posiciÃ³n aparece el tipo correcto?
- **Precision@k**: Â¿QuÃ© % de retrieved son relevantes?

### Generation Metrics
- **Answer Correctness**: Â¿La respuesta identifica los daÃ±os correctos?
- **Faithfulness**: Â¿La respuesta usa el contexto RAG?
- **Hallucination Rate**: Â¿Inventa daÃ±os que no existen?

### Performance Metrics
- **Retrieval Time**: Tiempo de bÃºsqueda FAISS
- **Generation Time**: Tiempo Qwen3VL
- **Total Latency**: End-to-end

---

## ğŸ¯ Resultados Esperados

### âœ… Con EvaluaciÃ³n Correcta

```
Test Set (543 imÃ¡genes nunca vistas):
  - Recall@5: 75-85% (realista)
  - MRR: 0.65-0.75 (realista)
  - Answer Correctness: 70-80%
```

**Estos son resultados REALES** que reflejan performance en producciÃ³n.

### âŒ Lo Que TenÃ­as Antes (Incorrecto)

```
Same images (1,024 crops):
  - Recall@5: 100% (trampa - mismo dato)
  - MRR: 1.0 (trampa - busca exactamente sÃ­ mismo)
```

---

## ğŸ’¡ Ejemplo Concreto

### Imagen de Test (NUNCA vista)

```
test/zona5_1234_original.jpg
Ground truth:
  - 3x surface_scratch
  - 1x dent
  - 1x paint_peeling
```

### Pipeline RAG

**1. Genera embedding** (DINOv3)
```python
query_emb = embedder.generate_embedding("test/zona5_1234_original.jpg")
# shape: (1024,)
```

**2. Retrieval FAISS** (busca en train set)
```python
results = retriever.search(query_emb, k=5)
# Recupera 5 crops mÃ¡s similares del TRAIN set
```

**3. Construye contexto RAG**
```
## Ejemplos Similares:

### Ejemplo 1 (similitud: 92%):
- Tipo: surface_scratch
- Zona: hood_center
- Imagen: train/zona2_5678_crop_045.jpg

### Ejemplo 2 (similitud: 88%):
- Tipo: dent
- Zona: front_left
...
```

**4. Genera respuesta con Qwen3VL**
```json
{
  "damages": [
    {"type": "surface_scratch", "location": "hood", "count": 3},
    {"type": "dent", "location": "door_left", "count": 1},
    {"type": "paint_peeling", "location": "bumper", "count": 1}
  ]
}
```

**5. EvalÃºa vs Ground Truth**
```
Recall@5: 100% (encontrÃ³ surface_scratch, dent, paint_peeling)
Answer Correctness: 100% (identificÃ³ todos correctamente)
```

---

## ğŸ”„ ComparaciÃ³n: Antes vs DespuÃ©s

| Aspecto | âŒ Antes (Incorrecto) | âœ… DespuÃ©s (Correcto) |
|---------|----------------------|----------------------|
| **Train/Test Split** | No | SÃ­ (80/20) |
| **Queries** | Mismo train set | Test set (no visto) |
| **Recall@5** | 100% (trampa) | 75-85% (realista) |
| **Genera respuesta VLM** | No | SÃ­ (Qwen3VL) |
| **EvalÃºa respuesta** | No | SÃ­ (vs ground truth) |
| **Refleja producciÃ³n** | No | SÃ­ |

---

## âœ¨ Bonus: EvaluaciÃ³n Humana

Para mÃ¡xima validez, aÃ±ade evaluaciÃ³n humana:

```bash
# Genera reporte con imÃ¡genes
python scripts/08_generate_visual_report.py \
    --results outputs/rag_evaluation/rag_evaluation_results.json \
    --output outputs/rag_evaluation/visual_report.html
```

Luego revisa manualmente 20-30 casos:
- Â¿La respuesta es correcta?
- Â¿El contexto RAG fue Ãºtil?
- Â¿Hay hallucinations?

---

## ğŸ“Œ Checklist Final

Antes de confiar en resultados RAG:

- [ ] Train/test split hecho (80/20)
- [ ] Ãndice FAISS generado SOLO del train set
- [ ] Test set NUNCA visto durante training
- [ ] EvaluaciÃ³n usa imÃ¡genes test como queries
- [ ] Pipeline genera embeddings on-the-fly
- [ ] Qwen3VL genera respuestas con contexto RAG
- [ ] MÃ©tricas calculadas vs ground truth
- [ ] Recall@5 es realista (70-85%, no 100%)

Si marcaste todo âœ…, tienes una **evaluaciÃ³n RAG vÃ¡lida** ğŸ‰

---

## ğŸš€ EjecuciÃ³n RÃ¡pida

```bash
# Todo en un script
./scripts/run_full_rag_evaluation.sh
```

O paso a paso:

```bash
# 1. Split (15 min)
python scripts/06_split_train_test.py

# 2. Train pipeline (1.5 hrs)
python scripts/02_generate_clustered_crops.py --dataset data/raw/train_test_split/train
python scripts/03_generate_embeddings_dinov3.py --metadata data/processed/crops/train_set/metadata/...
python scripts/04_build_faiss_index.py --embeddings data/processed/embeddings/train_set/...

# 3. Evaluate (1 hr)
python scripts/07_evaluate_rag_end_to_end.py --test-set data/raw/train_test_split/test
```